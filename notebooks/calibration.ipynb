{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaaea907-591f-4d6c-b393-7451b653a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../examples/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics import CalibrationError\n",
    "\n",
    "from models.initializer import initialize_torchvision_model, initialize_model\n",
    "from transforms import initialize_transform\n",
    "from utils import get_config\n",
    "import wilds\n",
    "from wilds.common.grouper import CombinatorialGrouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c5e1d0f-29bf-4185-aa29-946433879dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_loader(config):\n",
    "    full_dataset = wilds.get_dataset(\n",
    "        dataset=config.dataset,\n",
    "        version=config.version,\n",
    "        root_dir=config.root_dir,\n",
    "        download=False,\n",
    "        split_scheme=config.split_scheme,\n",
    "        **config.dataset_kwargs)\n",
    "    eval_transform = initialize_transform(\n",
    "        transform_name=config.transform,\n",
    "        config=config,\n",
    "        dataset=full_dataset,\n",
    "        is_training=False)\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=config.groupby_fields)\n",
    "    tst_dset = full_dataset.get_subset(\n",
    "        \"test\",\n",
    "        train_grouper=train_grouper,\n",
    "        frac=config.frac,\n",
    "        transform=eval_transform,\n",
    "        subsample_to_minority=config.subsample)\n",
    "    loader = DataLoader(\n",
    "        tst_dset,\n",
    "        shuffle=False, # Do not shuffle eval datasets\n",
    "        sampler=None,\n",
    "        collate_fn=tst_dset.collate,\n",
    "        batch_size=config.batch_size,\n",
    "        **config.loader_kwargs)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992053a5-42bb-4865-aac2-a1eb3da40fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"celebA\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "loader = get_dataset_loader(config)\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"resnet18\",\n",
    "    'model_path': \"../logs/celebA/erm/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO wd\",\n",
    "    \"arch\": \"resnet18\",\n",
    "    'model_path': \"../logs/celebA/groupDRO_wd1.0/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW\",\n",
    "    \"arch\": \"resnet18\",\n",
    "    'model_path': \"../logs/celebA/erm_reweight/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM DPSGD\",\n",
    "    \"arch\": \"dp_resnet18\",\n",
    "    'model_path': \"../logs/celebA/erm-dp_resnet18-dpsgd_1e-5_1.0_0.1_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW DPSGD\",\n",
    "    \"arch\": \"dp_resnet18\",\n",
    "    'model_path': \"../logs/celebA/iwerm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ef3928-ec51-4c30-87cc-7af3c5ab1e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0541dfbb37f4271be5b05abc0993e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d1b72775ac42efb5296de460225087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bece75c3d55452fada2f36aab1cb130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b008a0b0d65b4994ac5ac9d43d7bbeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6f7c0cb20f4ed9a7e752386bdec9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "results = {}\n",
    "for param in params:\n",
    "    name, arch, model_path = param['name'], param['arch'], param['model_path']\n",
    "\n",
    "    d_out = 2\n",
    "    model = initialize_torchvision_model(arch, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    proba, truths = [], []\n",
    "    for x, y, _ in tqdm(loader):\n",
    "        proba.append(torch.nn.Softmax(dim=1)(model(x.to(device))).detach().cpu())\n",
    "        truths.append(y)\n",
    "    proba = torch.cat(proba, dim=0)\n",
    "    truths = torch.cat(truths)\n",
    "    \n",
    "    error = CalibrationError()\n",
    "    error(proba, truths)\n",
    "    results[(dataset, name)] = error.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919cc6e7-aeff-46af-9e80-4f2329ccb49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM)</th>\n",
       "      <td>0.041986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, DRO wd)</th>\n",
       "      <td>0.366747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM IW)</th>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM DPSGD)</th>\n",
       "      <td>0.054780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM IW DPSGD)</th>\n",
       "      <td>0.055586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "(celebA, ERM)           0.041986\n",
       "(celebA, DRO wd)        0.366747\n",
       "(celebA, ERM IW)        0.101050\n",
       "(celebA, ERM DPSGD)     0.054780\n",
       "(celebA, ERM IW DPSGD)  0.055586"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a188e4-5679-417e-81bc-1df9bd3d89f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('celebA', 'ERM'): 0.04198582097887993,\n",
       " ('celebA', 'DRO wd'): 0.36674678325653076,\n",
       " ('celebA', 'ERM IW'): 0.10104991495609283,\n",
       " ('celebA', 'ERM DPSGD'): 0.05478046089410782,\n",
       " ('celebA', 'ERM IW DPSGD'): 0.0555860698223114}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2e54f39-0255-415f-b2bc-70e04eaec613",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"utkface\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "config.download = True\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/utkface/erm-resnet50/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/utkface/\",\n",
    "}, {\n",
    "    'name': \"DRO wd\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/civilcomments/groupDRO-head_bert-base-uncased_wd1.0/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/civilcomments/erm_reweight-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "#}, {\n",
    "#    'name': \"ERM IW DPSGD\",\n",
    "#    \"arch\": \"dp_bert-base-uncased\",\n",
    "#    'model_path': \"../logs/celebA/iwerm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/weightederm-dp_bert-base-uncased-dpsgd_1e-5_0.5_1.0_0.0001/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"IWERM DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/iwerm-dp_bert-base-uncased-lr1e-5_dpAdamW_1e-5_0.001_1.0_0.0002/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupdro-dp_bert-base-uncased-lr1e-5_dpAdamW_1e-5_0.001_1.0_0.0002/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78356b26-b48e-4c2e-ba15-35bf96705283",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6ad359fd641b09cc20e65412e0657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0923345b83f74628bc5599910288d46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for param in params:\n",
    "    name, arch, model_path = param['name'], param['arch'], param['model_path']\n",
    "    config.model = arch\n",
    "    loader = get_dataset_loader(config)\n",
    "    \n",
    "    d_out = 2\n",
    "    model = initialize_model(config, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    proba, truths = [], []\n",
    "    counts = 0\n",
    "    for x, y, _ in tqdm(loader):\n",
    "        proba.append(torch.nn.Softmax(dim=1)(model(x.to(device))).detach().cpu())\n",
    "        truths.append(y)\n",
    "        counts += 1\n",
    "        if counts == 100:\n",
    "            break\n",
    "    proba = torch.cat(proba, dim=0)\n",
    "    truths = torch.cat(truths)\n",
    "    \n",
    "    error = CalibrationError()\n",
    "    error(proba, truths)\n",
    "    results[(dataset, name)] = error.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abd40d-ae31-46b3-b284-f5e832295b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37b0c099-cd71-4b2b-b3da-ec71df44988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"civilcomments\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/erm-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupDRO-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO wd\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupDRO-head_bert-base-uncased_wd1.0/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/erm_reweight-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "#}, {\n",
    "#    'name': \"ERM IW DPSGD\",\n",
    "#    \"arch\": \"dp_bert-base-uncased\",\n",
    "#    'model_path': \"../logs/celebA/iwerm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/weightederm-dp_bert-base-uncased-dpsgd_1e-5_0.5_1.0_0.0001/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"IWERM DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/iwerm-dp_bert-base-uncased-lr1e-5_dpAdamW_1e-5_0.001_1.0_0.0002/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupdro-dp_bert-base-uncased-lr1e-5_dpAdamW_1e-5_0.001_1.0_0.0002/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf9f8e46-e6ef-4204-a694-6b9aff525e7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6ad359fd641b09cc20e65412e0657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0923345b83f74628bc5599910288d46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c3088cf6a14d2a8c7a370b41be18a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ebf93edc1e41cfa36d6f213564e255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f7e14924d24db0ad5a4be00ebac704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cba898267d44f709566f4f0838c3d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad025720e8e344f2bc94f338253e5127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for param in params:\n",
    "    name, arch, model_path = param['name'], param['arch'], param['model_path']\n",
    "    config.model = arch\n",
    "    loader = get_dataset_loader(config)\n",
    "    \n",
    "    d_out = 2\n",
    "    model = initialize_model(config, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    proba, truths = [], []\n",
    "    counts = 0\n",
    "    for x, y, _ in tqdm(loader):\n",
    "        proba.append(torch.nn.Softmax(dim=1)(model(x.to(device))).detach().cpu())\n",
    "        truths.append(y)\n",
    "        counts += 1\n",
    "        if counts == 100:\n",
    "            break\n",
    "    proba = torch.cat(proba, dim=0)\n",
    "    truths = torch.cat(truths)\n",
    "    \n",
    "    error = CalibrationError()\n",
    "    error(proba, truths)\n",
    "    results[(dataset, name)] = error.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04eeaa2a-e97c-4002-bcc2-dcc0ee9b38ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">celebA</th>\n",
       "      <th>ERM</th>\n",
       "      <td>0.041986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO wd</th>\n",
       "      <td>0.366747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW</th>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM DPSGD</th>\n",
       "      <td>0.054780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW DPSGD</th>\n",
       "      <td>0.055586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">civilcomments</th>\n",
       "      <th>ERM</th>\n",
       "      <td>0.107468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW</th>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO</th>\n",
       "      <td>0.150426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO wd</th>\n",
       "      <td>0.137926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW DPSGD</th>\n",
       "      <td>0.131587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO DPSGD</th>\n",
       "      <td>0.174024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IWERM DPSGD</th>\n",
       "      <td>0.572963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0\n",
       "celebA        ERM           0.041986\n",
       "              DRO wd        0.366747\n",
       "              ERM IW        0.101050\n",
       "              ERM DPSGD     0.054780\n",
       "              ERM IW DPSGD  0.055586\n",
       "civilcomments ERM           0.107468\n",
       "              ERM IW        0.130900\n",
       "              DRO           0.150426\n",
       "              DRO wd        0.137926\n",
       "              ERM IW DPSGD  0.131587\n",
       "              DRO DPSGD     0.174024\n",
       "              IWERM DPSGD   0.572963"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992fb34-372a-458b-ae87-86c3c631e62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
