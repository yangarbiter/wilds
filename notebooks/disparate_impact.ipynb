{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4adfe02-286e-47f9-b081-76269e36d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from opacus.accountants.rdp import RDPAccountant\n",
    "\n",
    "import wilds\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.rcParams['text.usetex'] = True #Let TeX do the typsetting\n",
    "plt.rcParams['text.latex.preamble'] = r\"\"\"\n",
    "\\usepackage{sansmath}\n",
    "\\sansmath\n",
    "\"\"\" #Force sans-serif math mode (for axes labels)\n",
    "plt.rcParams['font.family'] = 'sans-serif' # ... for regular text\n",
    "plt.rcParams['font.sans-serif'] = 'Helvetica, Avant Garde, Computer Modern Sans serif' # Choose a nice font here\n",
    "\n",
    "fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "6fa9bb58-43bf-477e-8a1f-f287b7a8b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus.accountants.analysis.gdp import compute_mu_poisson, eps_from_mu\n",
    "\n",
    "def get_sampling_weights(ds_name):\n",
    "    full_dataset = wilds.get_dataset(\n",
    "        dataset=ds_name,\n",
    "        version=\"1.0\",\n",
    "        root_dir=\"../data\",\n",
    "        download=True,\n",
    "        split_scheme=\"official\")\n",
    "\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "            dataset=full_dataset,\n",
    "            groupby_fields=full_dataset._metadata_fields)\n",
    "\n",
    "    trn_dset = full_dataset.get_subset(\n",
    "                \"train\",\n",
    "                train_grouper=train_grouper,\n",
    "                frac=1.0,\n",
    "                subsample_to_minority=False)\n",
    "\n",
    "    groups, group_counts = train_grouper.metadata_to_group(\n",
    "                    trn_dset.metadata_array,\n",
    "                    return_counts=True)\n",
    "    group_weights = 1 / group_counts\n",
    "    weights = group_weights[groups]\n",
    "    weights = weights / weights.sum() * len(trn_dset)\n",
    "    return weights, len(trn_dset)\n",
    "    \n",
    "def get_privacy_spent(sigma, epochs, n_iters, sample_rate, alphas,):\n",
    "    accountant = RDPAccountant()\n",
    "    for _ in range(epochs):\n",
    "        for _ in range(n_iters):\n",
    "            accountant.step(noise_multiplier=sigma*grad_norm, sample_rate=sample_rate)\n",
    "    return accountant.get_privacy_spent(delta=1e-5, alphas=alphas)\n",
    "\n",
    "def get_privacy_spent_v2(n_samples, sample_rate, sigma, epochs, weight):\n",
    "    #n_samples = 162_770\n",
    "    delta = delta=1 / (2 * n_samples)\n",
    "    mu = compute_mu_poisson(\n",
    "        steps=epochs / sample_rate,\n",
    "        sample_rate = sample_rate * weight,\n",
    "        noise_multiplier=sigma,\n",
    "    )\n",
    "    eps = eps_from_mu(mu=mu, delta=delta)\n",
    "    return eps\n",
    "\n",
    "def get_sigma_epsilon(ds_name, epochs, sample_rate, sigmas, grad_norm, weighted_sampling=True):\n",
    "    alphas = [1 + x / 2000.0 for x in range(1, 20000)] + list(range(12, 64))\n",
    "\n",
    "    if weighted_sampling:\n",
    "        weights, n_samples = get_sampling_weights(ds_name)\n",
    "        weight = weights.max().item()\n",
    "    else:\n",
    "        _, n_samples = get_sampling_weights(ds_name)\n",
    "        weight = 1.0\n",
    "\n",
    "    data = {}\n",
    "    for epoch, sigma in zip(epochs, sigmas):\n",
    "        #epsilon, alpha = get_privacy_spent(sigma, epoch, int(1 / sample_rate), sample_rate * weight, alphas)\n",
    "        epsilon = get_privacy_spent_v2(n_samples, sample_rate, sigma, epoch, weight)\n",
    "        data[sigma] = epsilon\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f518cd2e-02eb-4e17-880f-fabd1c8e7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res(log_path, col_names):\n",
    "    df = pd.read_csv(f\"{log_path}/train_eval.csv\")\n",
    "    trn_res = df[col_names].values.tolist()[-1]\n",
    "    df = pd.read_csv(f\"{log_path}/test_eval.csv\")\n",
    "    tst_res = df[col_names].values.tolist()[-1]\n",
    "    return trn_res, tst_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ddecb7aa-0e3d-4b2a-baf5-13c6a4c82338",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    (\"train\", \"not blond\", \"female\"),\n",
    "    (\"train\", \"not blond\", \"male\"),\n",
    "    (\"train\", \"blond\", \"female\"),\n",
    "    (\"train\", \"blond\", \"male\"),\n",
    "    (\"test\", \"not blond\", \"female\"),\n",
    "    (\"test\", \"not blond\", \"male\"),\n",
    "    (\"test\", \"blond\", \"female\"),\n",
    "    (\"test\", \"blond\", \"male\"),\n",
    "]\n",
    "order = ['ERM', 'IWERM', 'gDRO', 'DP ERM', 'DP ERM IW']\n",
    "def key_fn(x):\n",
    "    ret = []\n",
    "    for xi in x:\n",
    "        if xi in order:\n",
    "            ret.append(order.index(xi))\n",
    "        else:\n",
    "            ret.append(0 if xi == \"train\" else 1)\n",
    "    return ret\n",
    "\n",
    "#df = pd.DataFrame.from_dict(data).transpose()\n",
    "#df.columns = pd.MultiIndex.from_tuples(column_names)\n",
    "#df = df.stack(0)\n",
    "#df = df.sort_index(axis=0, level=[0, 1], key=key_fn)\n",
    "#print(df.to_latex(float_format=\"%.2f\", multirow=True).replace(\"llrrrr\", \"llcccc\").replace(\"{l}\", \"{c}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "3bf1df92-6585-4446-be44-538845c3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sigmas, log_paths, col_names, max_epoch=-1, early_stop=False, base_path=\"./data/\"):\n",
    "    #base_path = \"../logs/\"\n",
    "    \n",
    "    results = {}\n",
    "    for sigma, log_path in zip(sigmas, log_paths):\n",
    "        results.setdefault(\"sigma\", []).append(sigma)\n",
    "        \n",
    "        if early_stop:\n",
    "            df = pd.read_csv(os.path.join(base_path, f\"{ds_name}/{log_path}/val_eval.csv\"))\n",
    "            val_res = df[[\"epoch\", \"acc_avg\", \"acc_wg\"] + col_names].values\n",
    "            epoch_no = val_res[:max_epoch, 1].argmax()\n",
    "            results.setdefault(\"val epoch\", []).append(epoch_no + 1)\n",
    "        else:\n",
    "            epoch_no = max_epoch - 1\n",
    "            results.setdefault(\"val epoch\", []).append(epoch_no)\n",
    "\n",
    "        df = pd.read_csv(os.path.join(base_path, f\"{ds_name}/{log_path}/train_eval.csv\"))\n",
    "        train_res = df[[\"epoch\", \"acc_avg\", \"acc_wg\"] + col_names].values\n",
    "        if epoch_no > len(train_res):\n",
    "            epoch_no = -1\n",
    "            print(log_path, len(train_res))\n",
    "        results.setdefault(\"trn acc\", []).append(train_res[epoch_no][1])\n",
    "        results.setdefault(\"trn wg acc\", []).append(train_res[epoch_no][2])\n",
    "        disparity = np.max(train_res[epoch_no][4:]) - np.min(train_res[epoch_no][4:])\n",
    "        results.setdefault(\"trn disparity\", []).append(disparity)\n",
    "\n",
    "        df = pd.read_csv(os.path.join(base_path, f\"{ds_name}/{log_path}/test_eval.csv\"))\n",
    "        res = df[[\"epoch\", \"acc_avg\", \"acc_wg\", \"epsilon\"] + col_names].values.tolist()\n",
    "        results.setdefault(\"tst acc\", []).append(res[epoch_no][1])\n",
    "        results.setdefault(\"tst wg acc\", []).append(res[epoch_no][2])\n",
    "        results.setdefault(\"epsilon\", []).append(res[epoch_no][3])\n",
    "        disparity = np.max(res[-1][4:]) - np.min(res[epoch_no][4:])\n",
    "        results.setdefault(\"tst disparity\", []).append(disparity)\n",
    "\n",
    "    for k, v in results.items():\n",
    "        results[k] = np.array(v)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0d68f21b-0af7-412a-95eb-451294ce64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"celebA\"\n",
    "epochs = 30\n",
    "\n",
    "col_names = ['acc_y:notblond_male:0', 'acc_y:notblond_male:1', 'acc_y:blond_male:0', 'acc_y:blond_male:1']\n",
    "\n",
    "sigmas = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "sigmas = [0.35, 0.4, 0.5, 1.0, 5.0]\n",
    "log_paths = []\n",
    "#for sigma in sigmas:\n",
    "#    log_paths.append(f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_{sigma}_1.0_0.0001\")\n",
    "log_paths = [\n",
    "    #f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_0.1_1.0_0.0001\",\n",
    "    f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_0.35_1.0_0.0001\",\n",
    "    f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_0.4_1.0_0.0001\",\n",
    "    f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_0.5_1.0_0.0001\",\n",
    "    f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_1.0_1.0_0.0001\",\n",
    "    f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_5.0_1.0_0.0001\",\n",
    "    #f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_10.0_1.0_0.0001\",\n",
    "]\n",
    "    \n",
    "data = get_data(sigmas, log_paths, col_names, max_epoch=epochs, early_stop=False, base_path=\"../logs/\")\n",
    "df1 = pd.DataFrame.from_dict(data)\n",
    "\n",
    "sigmas = [1.0, 2.0, 3.0, 5.0, 10.0]\n",
    "log_paths = [\n",
    "    f\"weightederm-dp_resnet50-dpsgd_1e-5_1.0_0.1_0.0001\",\n",
    "    f\"weightederm-dp_resnet50-dpsgd_1e-5_2.0_0.1_0.0001\",\n",
    "    f\"weightederm-dp_resnet50-dpsgd_1e-5_3.0_0.1_0.0001\",\n",
    "    f\"weightederm-dp_resnet50-dpsgd_1e-5_5.0_0.1_0.0001\",\n",
    "    f\"weightederm-dp_resnet50-dpsgd_1e-5_10.0_0.1_0.0001\",\n",
    "]\n",
    "data = get_data(sigmas, log_paths, col_names, max_epoch=epochs, early_stop=False, base_path=\"../logs/\")\n",
    "df2 = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "fe1d0e9f-e41c-4db5-999b-4c30060eeeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eps': [19.308295030488967, 5.986238994520431, 1.6704210664616577, 0.25916170582883546, 0.0342371335515071], 'weps': [11.211052000565424, 3.8919350898153136, 2.3720271692939545, 1.3253629702536864, 0.6194620711495639], 'val epoch': [29, 29, 29, 29, 29], 'val epoch dpiw': [29, 29, 29, 29, 29], 'tst disparity': [0.7322347164154055, 0.715302646160126, 0.8302440494298935, 1.0, 1.0], 'tst disparity dpiw': [0.14462369680404663, 0.15380823612213135, 0.1383289098739624, 0.3144318461418152, 0.8349544405937195], 'tst acc': [0.9348261952400208, 0.931319534778595, 0.9172928333282472, 0.8667468428611755, 0.8667468428611755], 'tst acc dpiw': [0.8767157793045044, 0.8292756080627441, 0.8033263087272644, 0.6442240476608276, 0.1332531869411468], 'tst wg acc': [0.2555555701255798, 0.2722222208976745, 0.1555555611848831, 0.0, 0.0], 'tst wg acc dpiw': [0.7666666507720947, 0.7611111402511597, 0.7596549391746521, 0.5512939691543579, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"celebA\"\n",
    "sample_rate = 0.0001\n",
    "grad_norm = 1.0\n",
    "\n",
    "#df = pd.merge(df1[['sigma', 'tst disparity', 'tst acc', 'val epoch']],\n",
    "#              df2[['sigma', 'tst disparity', 'tst acc', 'val epoch']], on=\"sigma\")\n",
    "\n",
    "#eps = get_sigma_epsilon(ds_name, df1['val epoch'].tolist(), sample_rate, df1['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=False)\n",
    "eps = get_sigma_epsilon(ds_name, [epochs] * len(df1['sigma']), sample_rate, df1['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=False)\n",
    "eps = [eps[s] for s in df1['sigma'].tolist()]\n",
    "#weps = get_sigma_epsilon(ds_name, df2['val epoch'].tolist(), sample_rate, df2['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=True)\n",
    "weps = get_sigma_epsilon(ds_name, [epochs] * len(df2['sigma']), sample_rate, df2['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=True)\n",
    "weps = [weps[s] for s in df2['sigma'].tolist()]\n",
    "\n",
    "ret = {\n",
    "    \"eps\": eps,\n",
    "    \"weps\": weps,\n",
    "    \"val epoch\": df1['val epoch'].tolist(),\n",
    "    \"val epoch dpiw\": df2['val epoch'].tolist(),\n",
    "    \"tst disparity\": df1['tst disparity'].tolist(),\n",
    "    \"tst disparity dpiw\": df2['tst disparity'].tolist(),\n",
    "    \"tst acc\": df1['tst acc'].tolist(),\n",
    "    \"tst acc dpiw\": df2['tst acc'].tolist(),\n",
    "    \"tst wg acc\": df1['tst wg acc'].tolist(),\n",
    "    \"tst wg acc dpiw\": df2['tst wg acc'].tolist(),\n",
    "}\n",
    "joblib.dump(ret, f\"data/disparity_{ds_name}.pkl\")\n",
    "\n",
    "print(ret)\n",
    "\n",
    "#plt.plot(eps, df['tst disparity_x'].tolist(), label=\"DP\")\n",
    "#plt.plot(weps, df['tst disparity_y'].tolist(), label=\"DP IW\")\n",
    "#\n",
    "#plt.xticks(fontsize=fontsize)\n",
    "#plt.yticks(fontsize=fontsize)\n",
    "#plt.xscale(\"log\")\n",
    "#plt.xlabel(\"Privacy budget $\\epsilon$\", fontsize=fontsize)\n",
    "#plt.ylabel(\"Disparity\", fontsize=fontsize)\n",
    "#plt.legend(fontsize=fontsize, frameon=False)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(f\"./figs/disparity_{ds_name}.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "730853c2-4e95-4da3-9ec6-d60c2af2b6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigma</th>\n",
       "      <th>tst disparity_x</th>\n",
       "      <th>tst acc_x</th>\n",
       "      <th>val epoch_x</th>\n",
       "      <th>tst disparity_y</th>\n",
       "      <th>tst acc_y</th>\n",
       "      <th>val epoch_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866747</td>\n",
       "      <td>1</td>\n",
       "      <td>0.107840</td>\n",
       "      <td>0.844404</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866747</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866747</td>\n",
       "      <td>2</td>\n",
       "      <td>0.834954</td>\n",
       "      <td>0.866747</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sigma  tst disparity_x  tst acc_x  val epoch_x  tst disparity_y  tst acc_y  \\\n",
       "0    1.0              1.0   0.866747            1         0.107840   0.844404   \n",
       "1    5.0              1.0   0.866747            1         1.000000   0.866747   \n",
       "2   10.0              1.0   0.866747            2         0.834954   0.866747   \n",
       "\n",
       "   val epoch_y  \n",
       "0           19  \n",
       "1            1  \n",
       "2            4  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "83a7b5ab-af29-49bf-b8a8-2ca5ad55c1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.23596134720532"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ttt(n_samples, sample_rate, sigma, epochs, weight, grad_norm):\n",
    "    #n_samples = 162_770\n",
    "    delta = delta=1 / (2 * n_samples)\n",
    "    mu = compute_mu_poisson(\n",
    "        steps=epochs / sample_rate,\n",
    "        sample_rate = sample_rate * weight,\n",
    "        noise_multiplier=sigma * grad_norm,\n",
    "    )\n",
    "    eps = eps_from_mu(mu=mu, delta=delta)\n",
    "    return eps\n",
    "ttt(162770, 0.0001, 1.0, 20, 27, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2246718-a469-4cfb-a1d2-b32f88d98414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 2.318670801047918e+20\n",
      "1.0 0.058622211293315185\n",
      "10.0 0.00448333962223875\n",
      "100.0 0.0004472247760728694\n"
     ]
    }
   ],
   "source": [
    "from opacus.accountants.analysis.gdp import compute_mu_poisson\n",
    "grad_norm = 1.0\n",
    "for sigma in df['sigma'].tolist():\n",
    "    mu = compute_mu_poisson(\n",
    "        steps=epochs / sample_rate,\n",
    "        sample_rate=sample_rate,\n",
    "        noise_multiplier=sigma * grad_norm,\n",
    "    )\n",
    "    print(sigma, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47eae60-c788-4abe-98ef-5150afb83634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6068910a-3a3b-45a3-9046-203141126b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "90ec5b87-9d04-4fbc-864e-28508bb71260",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"utkface\"\n",
    "\n",
    "col_names = [\n",
    "    'acc_y:male_race:White',\n",
    "    'acc_y:male_race:Black',\n",
    "    'acc_y:male_race:Asian',\n",
    "    'acc_y:male_race:Indian',\n",
    "    'acc_y:male_race:Others',\n",
    "    'acc_y:female_race:White',\n",
    "    'acc_y:female_race:Black',\n",
    "    'acc_y:female_race:Asian',\n",
    "    'acc_y:female_race:Indian',\n",
    "    'acc_y:female_race:Others',\n",
    "]\n",
    "\n",
    "base_path = \"./data\"\n",
    "\n",
    "data = {}\n",
    "log_path = os.path.join(base_path, \"utkface/erm-resnet50/\")\n",
    "data[\"ERM\"] = np.concatenate(get_res(log_path, col_names))\n",
    "log_path = os.path.join(base_path, \"utkface/iwerm-resnet50/\")\n",
    "data[\"IWERM\"] = np.concatenate(get_res(log_path, col_names))\n",
    "log_path = os.path.join(base_path, \"utkface/erm-dp_resnet50-lr1e-3-dpsgd_1e-5_0.1_1.0_0.001/\")\n",
    "data[\"DP ERM\"] = np.concatenate(get_res(log_path, col_names))\n",
    "log_path = os.path.join(base_path, \"utkface/weightederm-dp_resnet50-lr1e-3-dpsgd_1e-5_0.1_1.0_0.001/\")\n",
    "data[\"DP ERM IW\"] = np.concatenate(get_res(log_path, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6adb35b7-f5e3-4c3e-8127-82520fde00a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llccccc|ccccc}\n",
      "\\toprule\n",
      "          &      & \\multicolumn{5}{c}{female} & \\multicolumn{5}{c}{male} \\\\\n",
      "          &      &  Asian & Black & Indian & Others & White & Asian & Black & Indian & Others & White \\\\\n",
      "\\midrule\n",
      "\\multirow{2}{*}{ERM} & train &   1.00 &  1.00 &   1.00 &   0.99 &  1.00 &  1.00 &  1.00 &   1.00 &   0.99 &  1.00 \\\\\n",
      "          & test &   0.82 &  0.85 &   0.89 &   0.84 &  0.89 &  0.87 &  0.96 &   0.93 &   0.93 &  0.95 \\\\\n",
      "\\cline{1-12}\n",
      "\\multirow{2}{*}{IWERM} & train &   1.00 &  1.00 &   1.00 &   0.99 &  1.00 &  1.00 &  1.00 &   1.00 &   0.99 &  1.00 \\\\\n",
      "          & test &   0.86 &  0.89 &   0.91 &   0.86 &  0.91 &  0.87 &  0.95 &   0.93 &   0.89 &  0.93 \\\\\n",
      "\\cline{1-12}\n",
      "\\multirow{2}{*}{DP ERM} & train &   0.87 &  0.89 &   0.94 &   0.90 &  0.90 &  0.86 &  0.96 &   0.92 &   0.85 &  0.91 \\\\\n",
      "          & test &   0.87 &  0.90 &   0.93 &   0.96 &  0.93 &  0.76 &  0.92 &   0.88 &   0.77 &  0.88 \\\\\n",
      "\\cline{1-12}\n",
      "\\multirow{2}{*}{DP ERM IW} & train &   0.87 &  0.90 &   0.93 &   0.95 &  0.88 &  0.89 &  0.95 &   0.92 &   0.85 &  0.89 \\\\\n",
      "          & test &   0.78 &  0.81 &   0.83 &   0.81 &  0.81 &  0.90 &  0.96 &   0.95 &   0.88 &  0.93 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = []\n",
    "for i in [\"train\", \"test\"]:\n",
    "    for j in [\"male\", \"female\"]:\n",
    "        for k in [\"White\", \"Black\", \"Asian\", \"Indian\", \"Others\"]:\n",
    "            column_names.append((i, j, k))\n",
    "            \n",
    "order = ['ERM', 'IWERM', 'DP ERM', 'DP ERM IW']\n",
    "def key_fn(x):\n",
    "    ret = []\n",
    "    for xi in x:\n",
    "        if xi in order:\n",
    "            ret.append(order.index(xi))\n",
    "        else:\n",
    "            ret.append(0 if xi == \"train\" else 1)\n",
    "    return ret\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).transpose()\n",
    "df.columns = pd.MultiIndex.from_tuples(column_names)\n",
    "df = df.stack(0)\n",
    "df = df.sort_index(axis=0, level=[0, 1], key=key_fn)\n",
    "print(df.to_latex(float_format=\"%.2f\", multirow=True).replace(\"llrrrrrrrrrr\", \"llccccc|ccccc\").replace(\"{l}\", \"{c}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cdcf3ac5-7ee1-4ae5-9a57-fce55e80e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"utkface\"\n",
    "epochs = 100\n",
    "\n",
    "col_names = [\n",
    "    'acc_y:male_race:White',\n",
    "    'acc_y:male_race:Black',\n",
    "    'acc_y:male_race:Asian',\n",
    "    'acc_y:male_race:Indian',\n",
    "    'acc_y:male_race:Others',\n",
    "    'acc_y:female_race:White',\n",
    "    'acc_y:female_race:Black',\n",
    "    'acc_y:female_race:Asian',\n",
    "    'acc_y:female_race:Indian',\n",
    "    'acc_y:female_race:Others',\n",
    "]\n",
    "\n",
    "sigmas = [0.5, 10.0]\n",
    "log_paths = []\n",
    "#for sigma in sigmas:\n",
    "#    log_paths.append(f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_{sigma}_1.0_0.001\")\n",
    "log_paths.append(f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_1.0_1.0_0.001\")\n",
    "log_paths.append(f\"erm-dp_resnet50-lr1e-3-dpsgd_1e-5_10.0_1.0_0.001\")\n",
    "data = get_data(sigmas, log_paths, col_names, max_epoch=epochs, early_stop=False, base_path=\"../logs/\")\n",
    "df1 = pd.DataFrame.from_dict(data)\n",
    "\n",
    "sigmas = [1.0, 10.0]\n",
    "log_paths = []\n",
    "for sigma in sigmas:\n",
    "    log_paths.append(f\"weightederm-dp_resnet50-lr1e-3-dpsgd_1e-5_{sigma}_1.0_0.001\")\n",
    "data = get_data(sigmas, log_paths, col_names, max_epoch=epochs, early_stop=False, base_path=\"../logs/\")\n",
    "df2 = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2101112d-b183-44b4-83c9-4771ba261f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to ../data/UTKFace_v1.0...\n",
      "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
      "Downloading  to ../data/UTKFace_v1.0/archive.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c54445dde4a41718d70e329844dadfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0Byte [00:00, ?Byte/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/UTKFace_v1.0/archive.tar.gz may be corrupted. Please try deleting it and rerunning this command.\n",
      "\n",
      "Exception:  unknown url type: ''\n",
      "problem with:  ../data/UTKFace_v1.0/39_1_20170116174525125.jpg.chip.jpg\n",
      "problem with:  ../data/UTKFace_v1.0/61_1_20170109142408075.jpg.chip.jpg\n",
      "problem with:  ../data/UTKFace_v1.0/61_1_20170109150557335.jpg.chip.jpg\n",
      "Downloading dataset to ../data/UTKFace_v1.0...\n",
      "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
      "Downloading  to ../data/UTKFace_v1.0/archive.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6104429235bb4307a7d053e0e9280003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0Byte [00:00, ?Byte/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/UTKFace_v1.0/archive.tar.gz may be corrupted. Please try deleting it and rerunning this command.\n",
      "\n",
      "Exception:  unknown url type: ''\n",
      "problem with:  ../data/UTKFace_v1.0/39_1_20170116174525125.jpg.chip.jpg\n",
      "problem with:  ../data/UTKFace_v1.0/61_1_20170109142408075.jpg.chip.jpg\n",
      "problem with:  ../data/UTKFace_v1.0/61_1_20170109150557335.jpg.chip.jpg\n",
      "{'eps': [11.40438095741977, 0.08729803886149753], 'weps': [5.55425476644115, 0.30655830788646443], 'val epoch': [99, 99], 'val epoch dpiw': [99, 99], 'tst disparity': [0.2614583373069763, 0.6993197053670883], 'tst disparity dpiw': [0.2925248146057129, 0.6915723532438278], 'tst acc': [0.716652512550354, 0.5278249979019165], 'tst acc dpiw': [0.6977485418319702, 0.5178419947624207], 'tst wg acc': [0.5718749761581421, 0.1695692092180252], 'tst wg acc dpiw': [0.5289855003356934, 0.1577726155519485]}\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"utkface\"\n",
    "sample_rate = 0.001\n",
    "\n",
    "#df = pd.merge(df1[['sigma', 'tst disparity']], df2[['sigma', 'tst disparity']], on=\"sigma\")\n",
    "#\n",
    "#eps = get_sigma_epsilon(ds_name, epochs, sample_rate, df['sigma'].tolist(), False)\n",
    "#eps = [eps[s] for s in df['sigma'].tolist()]\n",
    "#weps = get_sigma_epsilon(ds_name, epochs, sample_rate, df['sigma'].tolist(), True)\n",
    "#weps = [weps[s] for s in df['sigma'].tolist()]\n",
    "\n",
    "eps = get_sigma_epsilon(ds_name, [epochs] * len(df1['sigma']), sample_rate, df1['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=False)\n",
    "eps = [eps[s] for s in df1['sigma'].tolist()]\n",
    "weps = get_sigma_epsilon(ds_name, [epochs] * len(df2['sigma']), sample_rate, df2['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=True)\n",
    "weps = [weps[s] for s in df2['sigma'].tolist()]\n",
    "\n",
    "ret = {\n",
    "    \"eps\": eps,\n",
    "    \"weps\": weps,\n",
    "    \"val epoch\": df1['val epoch'].tolist(),\n",
    "    \"val epoch dpiw\": df2['val epoch'].tolist(),\n",
    "    \"tst disparity\": df1['tst disparity'].tolist(),\n",
    "    \"tst disparity dpiw\": df2['tst disparity'].tolist(),\n",
    "    \"tst acc\": df1['tst acc'].tolist(),\n",
    "    \"tst acc dpiw\": df2['tst acc'].tolist(),\n",
    "    \"tst wg acc\": df1['tst wg acc'].tolist(),\n",
    "    \"tst wg acc dpiw\": df2['tst wg acc'].tolist(),\n",
    "}\n",
    "joblib.dump(ret, f\"data/disparity_{ds_name}.pkl\")\n",
    "print(ret)\n",
    "\n",
    "#plt.plot(eps, df['tst disparity_x'].tolist(), label=\"DP\")\n",
    "#plt.plot(weps, df['tst disparity_y'].tolist(), label=\"DP IW\")\n",
    "#\n",
    "#plt.xticks(fontsize=fontsize)\n",
    "#plt.yticks(fontsize=fontsize)\n",
    "#plt.xscale(\"log\")\n",
    "#plt.xlabel(\"Privacy budget $\\epsilon$\", fontsize=fontsize)\n",
    "#plt.ylabel(\"Disparity\", fontsize=fontsize)\n",
    "#plt.legend(fontsize=fontsize, frameon=False)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(f\"./figs/disparity_{ds_name}.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ede03e-47c4-48a0-b1d1-21a9cfa2abf7",
   "metadata": {},
   "source": [
    "# iNaturalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c6734a66-4942-4c21-a264-9149010b9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"inaturalist\"\n",
    "epochs = 20\n",
    "\n",
    "col_names = [\n",
    "    'acc_y:Actinopterygii',\n",
    "    'acc_y:Amphibia',\n",
    "    'acc_y:Animalia',\n",
    "    'acc_y:Arachnida',\n",
    "    'acc_y:Aves', 'acc_y:Chromista',\n",
    "    'acc_y:Fungi', 'acc_y:Insecta',\n",
    "    'acc_y:Mammalia', 'acc_y:Mollusca',\n",
    "    'acc_y:Plantae',\n",
    "    'acc_y:Protozoa',\n",
    "    'acc_y:Reptilia',\n",
    "]\n",
    "\n",
    "sigmas = [1.0]\n",
    "log_paths = []\n",
    "for sigma in sigmas:\n",
    "    log_paths.append(f\"erm-dp_resnet18-lr1e-3-dpsgd_1e-5_{sigma}_1.0_0.0001\")\n",
    "data = get_data(sigmas, log_paths, col_names, max_epoch=epochs, early_stop=False, base_path=\"../logs/\")\n",
    "df1 = pd.DataFrame.from_dict(data)\n",
    "\n",
    "sigmas = [0.8]\n",
    "log_paths = []\n",
    "log_paths.append(f\"weightederm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001\")\n",
    "#for sigma in sigmas:\n",
    "#    log_paths.append(f\"weightederm-dp_resnet18-dpsgd_1e-5_{sigma}_1.0_0.0001\")\n",
    "data = get_data(sigmas, log_paths, col_names, max_epoch=epochs, early_stop=False, base_path=\"../logs/\")\n",
    "df2 = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "29f40d2d-d644-4f26-92f9-16cf4749fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to ../data/inaturalist_v1.0...\n",
      "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
      "Using downloaded and verified file: ../data/inaturalist_v1.0/archive.tar.gz\n",
      "Extracting ../data/inaturalist_v1.0/archive.tar.gz to ../data/inaturalist_v1.0\n",
      "\n",
      "../data/inaturalist_v1.0/archive.tar.gz may be corrupted. Please try deleting it and rerunning this command.\n",
      "\n",
      "Exception:  Compressed file ended before the end-of-stream marker was reached\n",
      "Downloading dataset to ../data/inaturalist_v1.0...\n",
      "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
      "Using downloaded and verified file: ../data/inaturalist_v1.0/archive.tar.gz\n",
      "Extracting ../data/inaturalist_v1.0/archive.tar.gz to ../data/inaturalist_v1.0\n",
      "\n",
      "../data/inaturalist_v1.0/archive.tar.gz may be corrupted. Please try deleting it and rerunning this command.\n",
      "\n",
      "Exception:  Compressed file ended before the end-of-stream marker was reached\n",
      "{'eps': [0.22558761297439495], 'weps': [126.59619444108074], 'val epoch': [19], 'val epoch dpiw': [19], 'tst disparity': [0.8318488001823425], 'tst disparity dpiw': [0.5732691138982773], 'tst acc': [0.6251031160354614], 'tst acc dpiw': [0.4188772141933441], 'tst wg acc': [0.0], 'tst wg acc dpiw': [0.1721854358911514]}\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"inaturalist\"\n",
    "sample_rate = 0.0001\n",
    "\n",
    "eps = get_sigma_epsilon(ds_name, [epochs] * len(df1['sigma']), sample_rate, df1['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=False)\n",
    "eps = [eps[s] for s in df1['sigma'].tolist()]\n",
    "weps = get_sigma_epsilon(ds_name, [epochs] * len(df2['sigma']), sample_rate, df2['sigma'].tolist(), grad_norm=grad_norm, weighted_sampling=True)\n",
    "weps = [weps[s] for s in df2['sigma'].tolist()]\n",
    "\n",
    "ret = {\n",
    "    \"eps\": eps,\n",
    "    \"weps\": weps,\n",
    "    \"val epoch\": df1['val epoch'].tolist(),\n",
    "    \"val epoch dpiw\": df2['val epoch'].tolist(),\n",
    "    \"tst disparity\": df1['tst disparity'].tolist(),\n",
    "    \"tst disparity dpiw\": df2['tst disparity'].tolist(),\n",
    "    \"tst acc\": df1['tst acc'].tolist(),\n",
    "    \"tst acc dpiw\": df2['tst acc'].tolist(),\n",
    "    \"tst wg acc\": df1['tst wg acc'].tolist(),\n",
    "    \"tst wg acc dpiw\": df2['tst wg acc'].tolist(),\n",
    "}\n",
    "joblib.dump(ret, f\"data/disparity_{ds_name}.pkl\")\n",
    "print(ret)\n",
    "\n",
    "#plt.plot(eps, df['tst disparity_x'].tolist(), label=\"DP\")\n",
    "#plt.plot(weps, df['tst disparity_y'].tolist(), label=\"DP IW\")\n",
    "#\n",
    "#plt.xticks(fontsize=fontsize)\n",
    "#plt.yticks(fontsize=fontsize)\n",
    "#plt.xscale(\"log\")\n",
    "#plt.xlabel(\"Privacy budget $\\epsilon$\", fontsize=fontsize)\n",
    "#plt.ylabel(\"Disparity\", fontsize=fontsize)\n",
    "#plt.legend(fontsize=fontsize)\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(f\"./figs/disparity_{ds_name}.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ec527-3924-4161-8a50-860ce84ad4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
