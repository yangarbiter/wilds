{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaea907-591f-4d6c-b393-7451b653a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../examples/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics import CalibrationError\n",
    "\n",
    "from models.initializer import initialize_torchvision_model, initialize_model\n",
    "from transforms import initialize_transform\n",
    "from utils import get_config\n",
    "import wilds\n",
    "from wilds.common.grouper import CombinatorialGrouper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c5e1d0f-29bf-4185-aa29-946433879dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_loader(config):\n",
    "    full_dataset = wilds.get_dataset(\n",
    "        dataset=config.dataset,\n",
    "        version=config.version,\n",
    "        root_dir=config.root_dir,\n",
    "        download=True,\n",
    "        split_scheme=config.split_scheme,\n",
    "        **config.dataset_kwargs)\n",
    "    eval_transform = initialize_transform(\n",
    "        transform_name=config.transform,\n",
    "        config=config,\n",
    "        dataset=full_dataset,\n",
    "        is_training=False)\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=config.groupby_fields)\n",
    "    \n",
    "    trn_dset = full_dataset.get_subset(\n",
    "        \"train\",\n",
    "        train_grouper=train_grouper,\n",
    "        frac=config.frac,\n",
    "        transform=eval_transform,\n",
    "        subsample_to_minority=config.subsample)\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dset,\n",
    "        shuffle=False, # Do not shuffle eval datasets\n",
    "        sampler=None,\n",
    "        collate_fn=trn_dset.collate,\n",
    "        batch_size=config.batch_size,\n",
    "        **config.loader_kwargs)\n",
    "    tst_dset = full_dataset.get_subset(\n",
    "        \"test\",\n",
    "        train_grouper=train_grouper,\n",
    "        frac=config.frac,\n",
    "        transform=eval_transform,\n",
    "        subsample_to_minority=config.subsample)\n",
    "    tst_loader = DataLoader(\n",
    "        tst_dset,\n",
    "        shuffle=False, # Do not shuffle eval datasets\n",
    "        sampler=None,\n",
    "        collate_fn=tst_dset.collate,\n",
    "        batch_size=config.batch_size,\n",
    "        **config.loader_kwargs)\n",
    "    return trn_loader, tst_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1803a832-6be0-4e43-8e34-e499eb8b3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"celebA\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "trn_loader, tst_loader = get_dataset_loader(config)\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/celebA/erm-resnet50/celebA_seed:0_epoch:last_model.pth\",\n",
    "    'pred_dir': \"../logs/celebA/erm-resnet50/\",\n",
    "}]\n",
    "params += [{\n",
    "    'name': \"ERM DPSGD\",\n",
    "    \"arch\": \"dp_resnet50\",\n",
    "    'model_path': f\"../logs/celebA/erm-dp_resnet50-lr1e-3-dpsgd_1e-5_{gamma}_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "    'pred_dir': f\"../logs/celebA/erm-dp_resnet50-lr1e-3-dpsgd_1e-5_{gamma}_1.0_0.0001/\",\n",
    "} for gamma in [0.0001, 0.001, 0.01, 0.1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22464b3e-e6a7-42fc-bdd5-aa85fbe3326a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193c06ba72c849d7b208d031351fbba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 10.76 GiB total capacity; 959.36 MiB already allocated; 120.44 MiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9093f8bb68e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"trn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tst\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/spurious_ml/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 10.76 GiB total capacity; 959.36 MiB already allocated; 120.44 MiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for param in params:\n",
    "    name, arch, model_path, pred_dir = param['name'], param['arch'], param['model_path'], param['pred_dir']\n",
    "\n",
    "    d_out = 2\n",
    "    model = initialize_torchvision_model(arch, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    ret = {\"trn\": [], \"tst\": []}\n",
    "    for x, y, _ in tqdm(trn_loader):\n",
    "        ret[\"trn\"].append(model(x.to(device)).detach().cpu())\n",
    "    ret[\"trn\"] = torch.cat(ret[\"trn\"], dim=0)\n",
    "    for x, y, _ in tqdm(tst_loader):\n",
    "        ret[\"tst\"].append(model(x.to(device)).detach().cpu())\n",
    "    ret[\"tst\"] = torch.cat(ret[\"tst\"], dim=0)\n",
    "    \n",
    "    joblib.dump(ret, os.path.join(pred_dir, \"preds.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5772108-8952-427b-a019-361101932304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e374b-0966-4eb1-bfe5-3a6a6485b4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a8533-d51d-4bab-8e3f-8510cbaa6fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a21c3e-287c-4309-b408-bfc2b9c9d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992053a5-42bb-4865-aac2-a1eb3da40fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"celebA\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "loader = get_dataset_loader(config)\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/celebA/erm/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/celebA/groupDRO/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO wd\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/celebA/groupDRO_wd0.1/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW\",\n",
    "    \"arch\": \"resnet18\",\n",
    "    'model_path': \"../logs/celebA/erm_reweight/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM DPSGD\",\n",
    "    \"arch\": \"dp_resnet18\",\n",
    "    'model_path': \"../logs/celebA/erm-dp_resnet18-dpsgd_1e-5_1.0_0.1_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW DPSGD\",\n",
    "    \"arch\": \"dp_resnet18\",\n",
    "    'model_path': \"../logs/celebA/iwerm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fa59e-f1a1-4064-b93d-a62c1f5aa2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ef3928-ec51-4c30-87cc-7af3c5ab1e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d210a436e746deaaa2f180eb0f1414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70632c9de98643c4a8ccc5ce5eda20ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58531f24a02468182be76c2f2b06e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7328ac440b6a474795414e67ea308cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efbc2b75675458e84b6fb448bc1a5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4516d0d7267041b0aeab6b16ae37f05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for param in params:\n",
    "    name, arch, model_path = param['name'], param['arch'], param['model_path']\n",
    "\n",
    "    d_out = 2\n",
    "    model = initialize_torchvision_model(arch, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    proba, truths = [], []\n",
    "    for x, y, _ in tqdm(loader):\n",
    "        proba.append(torch.nn.Softmax(dim=1)(model(x.to(device))).detach().cpu())\n",
    "        truths.append(y)\n",
    "    proba = torch.cat(proba, dim=0)\n",
    "    truths = torch.cat(truths)\n",
    "    \n",
    "    error = CalibrationError()\n",
    "    error(proba, truths)\n",
    "    results[(dataset, name)] = error.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919cc6e7-aeff-46af-9e80-4f2329ccb49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM)</th>\n",
       "      <td>0.041986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, DRO wd)</th>\n",
       "      <td>0.366747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM IW)</th>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM DPSGD)</th>\n",
       "      <td>0.054780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(celebA, ERM IW DPSGD)</th>\n",
       "      <td>0.055586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "(celebA, ERM)           0.041986\n",
       "(celebA, DRO wd)        0.366747\n",
       "(celebA, ERM IW)        0.101050\n",
       "(celebA, ERM DPSGD)     0.054780\n",
       "(celebA, ERM IW DPSGD)  0.055586"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a188e4-5679-417e-81bc-1df9bd3d89f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('celebA', 'ERM'): 0.04198582097887993,\n",
       " ('celebA', 'DRO wd'): 0.36674678325653076,\n",
       " ('celebA', 'ERM IW'): 0.10104991495609283,\n",
       " ('celebA', 'ERM DPSGD'): 0.05478046089410782,\n",
       " ('celebA', 'ERM IW DPSGD'): 0.0555860698223114}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e54f39-0255-415f-b2bc-70e04eaec613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to ../data/UTKFace_v1.0...\n",
      "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
      "Downloading  to ../data/UTKFace_v1.0/archive.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c259c5dac59426e9e6bc42931028b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0Byte [00:00, ?Byte/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/UTKFace_v1.0/archive.tar.gz may be corrupted. Please try deleting it and rerunning this command.\n",
      "\n",
      "Exception:  unknown url type: ''\n",
      "problem with:  ../data/UTKFace_v1.0/39_1_20170116174525125.jpg.chip.jpg\n",
      "problem with:  ../data/UTKFace_v1.0/61_1_20170109142408075.jpg.chip.jpg\n",
      "problem with:  ../data/UTKFace_v1.0/61_1_20170109150557335.jpg.chip.jpg\n"
     ]
    }
   ],
   "source": [
    "dataset = \"utkface\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "config.download = True\n",
    "loader = get_dataset_loader(config)\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/utkface/erm-resnet50/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/utkface/groupDRO-resnet50/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO wd\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/utkface/groupDRO-resnet50_wd0.1/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW\",\n",
    "    \"arch\": \"resnet50\",\n",
    "    'model_path': \"../logs/utkface/erm_reweight-resnet50/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "#}, {\n",
    "#    'name': \"ERM IW DPSGD\",\n",
    "#    \"arch\": \"dp_bert-base-uncased\",\n",
    "#    'model_path': \"../logs/celebA/iwerm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM DPSGD\",\n",
    "    \"arch\": \"dp_resnet50\",\n",
    "    'model_path': \"../logs/utkface/erm-dp_resnet50-dpsgd_1e-5_0.5_1.0_0.0005/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW DPSGD\",\n",
    "    \"arch\": \"dp_resnet50\",\n",
    "    'model_path': \"../logs/utkface/weightederm-dp_resnet50-dpsgd_1e-5_0.01_1.0_0.001/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO DPSGD\",\n",
    "    \"arch\": \"dp_resnet50\",\n",
    "    'model_path': \"../logs/utkface/groupdro-dp_resnet50-dpsgd_1e-5_0.01_1.0_0.001/UTKFace_seed:0_epoch:last_model.pth\",\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78356b26-b48e-4c2e-ba15-35bf96705283",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b39e1b964b4be0b4633193c581668a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22a914c62ae4e35a37b719166c00f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5dfadb69a64336b42e8c4588b08b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc678c4ec97479abce59ed40e6615bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac674a673604e4291c2ad22d3b451aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8953e43f82401dacc0344302180c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe55e7937024b7da2f55b2c3854042e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for param in params:\n",
    "    name, arch, model_path = param['name'], param['arch'], param['model_path']\n",
    "    config.model = arch\n",
    "    \n",
    "    \n",
    "    d_out = 2\n",
    "    model = initialize_model(config, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    proba, truths = [], []\n",
    "    for x, y, _ in tqdm(loader):\n",
    "        proba.append(torch.nn.Softmax(dim=1)(model(x.to(device))).detach().cpu())\n",
    "        truths.append(y)\n",
    "    proba = torch.cat(proba, dim=0)\n",
    "    truths = torch.cat(truths)\n",
    "    \n",
    "    error = CalibrationError()\n",
    "    error(proba, truths)\n",
    "    results[(dataset, name)] = error.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abd40d-ae31-46b3-b284-f5e832295b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b0c099-cd71-4b2b-b3da-ec71df44988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"civilcomments\"\n",
    "config = get_config(dataset, \"ERM\", \"../data\")\n",
    "\n",
    "params = [{\n",
    "    'name': \"ERM\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/erm-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupDRO-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO wd\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupDRO-head_bert-base-uncased_wd1.0/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW\",\n",
    "    \"arch\": \"head_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/erm_reweight-head_bert-base-uncased/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "#}, {\n",
    "#    'name': \"ERM IW DPSGD\",\n",
    "#    \"arch\": \"dp_bert-base-uncased\",\n",
    "#    'model_path': \"../logs/celebA/iwerm-dp_resnet18-dpsgd_1e-5_1.0_1.0_0.0001/celebA_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"ERM IW DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/weightederm-dp_bert-base-uncased-dpsgd_1e-5_0.5_1.0_0.0001/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"IWERM DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/iwerm-dp_bert-base-uncased-lr1e-5_dpAdamW_1e-5_0.001_1.0_0.0002/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "}, {\n",
    "    'name': \"DRO DPSGD\",\n",
    "    \"arch\": \"dp_bert-base-uncased\",\n",
    "    'model_path': \"../logs/civilcomments/groupdro-dp_bert-base-uncased-lr1e-5_dpAdamW_1e-5_0.001_1.0_0.0002/civilcomments_seed:0_epoch:last_model.pth\",\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9f8e46-e6ef-4204-a694-6b9aff525e7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b2f15a4fa44970917291af202a763c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c64dbbf0e047148080ebeefc149514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1befc949f1dc48c599f620c4972d0e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541a729c85a94cbd8191af25dfcbae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c73a17947040e8b5f757d2ffd19f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dde36f932e428bbf1bbe699ba1ef74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertClassifier: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae13aa3bece49608d8833fa976b8043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for param in params:\n",
    "    name, arch, model_path = param['name'], param['arch'], param['model_path']\n",
    "    config.model = arch\n",
    "    loader = get_dataset_loader(config)\n",
    "    \n",
    "    d_out = 2\n",
    "    model = initialize_model(config, d_out)\n",
    "    res = torch.load(model_path)['algorithm']\n",
    "    state_dict = {}\n",
    "    for k, v in res.items():\n",
    "        if \"dp\" in arch:\n",
    "            state_dict[k.replace(\"model._module.\", \"\")] = v\n",
    "        else:\n",
    "            state_dict[k.replace(\"model.\", \"\")] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "    _ = model.to(device)\n",
    "    \n",
    "    proba, truths = [], []\n",
    "    counts = 0\n",
    "    for x, y, _ in tqdm(loader):\n",
    "        proba.append(torch.nn.Softmax(dim=1)(model(x.to(device))).detach().cpu())\n",
    "        truths.append(y)\n",
    "        counts += 1\n",
    "        if counts == 1000:\n",
    "            break\n",
    "    proba = torch.cat(proba, dim=0)\n",
    "    truths = torch.cat(truths)\n",
    "    \n",
    "    error = CalibrationError()\n",
    "    error(proba, truths)\n",
    "    results[(dataset, name)] = error.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04eeaa2a-e97c-4002-bcc2-dcc0ee9b38ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">utkface</th>\n",
       "      <th>ERM</th>\n",
       "      <td>0.020405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO</th>\n",
       "      <td>0.060203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO wd</th>\n",
       "      <td>0.068113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW</th>\n",
       "      <td>0.016250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM DPSGD</th>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW DPSGD</th>\n",
       "      <td>0.060824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO DPSGD</th>\n",
       "      <td>0.068309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">civilcomments</th>\n",
       "      <th>ERM</th>\n",
       "      <td>0.037293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO</th>\n",
       "      <td>0.069121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO wd</th>\n",
       "      <td>0.062794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW</th>\n",
       "      <td>0.065083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW DPSGD</th>\n",
       "      <td>0.151231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IWERM DPSGD</th>\n",
       "      <td>0.241695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO DPSGD</th>\n",
       "      <td>0.093338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">celebA</th>\n",
       "      <th>ERM</th>\n",
       "      <td>0.041986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO</th>\n",
       "      <td>0.103176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO wd</th>\n",
       "      <td>0.085320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW</th>\n",
       "      <td>0.101050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM DPSGD</th>\n",
       "      <td>0.054780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW DPSGD</th>\n",
       "      <td>0.055586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0\n",
       "utkface       ERM           0.020405\n",
       "              DRO           0.060203\n",
       "              DRO wd        0.068113\n",
       "              ERM IW        0.016250\n",
       "              ERM DPSGD     0.026500\n",
       "              ERM IW DPSGD  0.060824\n",
       "              DRO DPSGD     0.068309\n",
       "civilcomments ERM           0.037293\n",
       "              DRO           0.069121\n",
       "              DRO wd        0.062794\n",
       "              ERM IW        0.065083\n",
       "              ERM IW DPSGD  0.151231\n",
       "              IWERM DPSGD   0.241695\n",
       "              DRO DPSGD     0.093338\n",
       "celebA        ERM           0.041986\n",
       "              DRO           0.103176\n",
       "              DRO wd        0.085320\n",
       "              ERM IW        0.101050\n",
       "              ERM DPSGD     0.054780\n",
       "              ERM IW DPSGD  0.055586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e992fb34-372a-458b-ae87-86c3c631e62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">utkface</th>\n",
       "      <th>ERM</th>\n",
       "      <td>0.020405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO</th>\n",
       "      <td>0.060203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO wd</th>\n",
       "      <td>0.068113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW</th>\n",
       "      <td>0.016250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM DPSGD</th>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERM IW DPSGD</th>\n",
       "      <td>0.060824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRO DPSGD</th>\n",
       "      <td>0.068309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0\n",
       "utkface ERM           0.020405\n",
       "        DRO           0.060203\n",
       "        DRO wd        0.068113\n",
       "        ERM IW        0.016250\n",
       "        ERM DPSGD     0.026500\n",
       "        ERM IW DPSGD  0.060824\n",
       "        DRO DPSGD     0.068309"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
